# -*- coding: utf-8 -*-
"""C√≥pia de explora√ß√£o_socio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LlfL5aTb5YVKy8chvJLptzBSoxmhXudN

#**Importing libraries**
"""

import pandas as pd
import seaborn as sns
import numpy as np
from matplotlib import pyplot as plt
import arviz as az 
import pymc3 as pm 
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn import linear_model

"""#**Importing data**"""

socio = pd.read_csv("socioambiental_mun.csv", sep=';', encoding="latin1", low_memory=False,)

dengue = pd.read_csv("prevalencia_2007_2017_mun.csv", sep=';', low_memory=False,)

pop = pd.read_excel("estimativa_municipios_TCU_2015.ods",
                             sheet_name="Munic√≠pios", header=1).dropna()[["COD. UF", "NOME DO MUNIC√çPIO", "POPULA√á√ÉO ESTIMADA"]]
pop = pop.loc[pop['COD. UF'] == 27.0]

# setting float values
socio['lixo']    = socio['lixo'].apply(lambda x: float(str(x).replace(',', '.')))
socio['esgoto']  = socio['esgoto'].apply(lambda x: float(str(x).replace(',', '.')))
socio['estudo']  = socio['estudo'].apply(lambda x: float(str(x).replace(',', '.')))
socio['salario'] = socio['salario'].apply(lambda x: float(str(x).replace(',', '.')))

# solving na values
socio.drop(socio.index[-1], axis=0, inplace=True)
socio.fillna(socio['estudo'].mean(), inplace=True)

"""# **Merging the datas**

Now we have data about socio-environmental, estimated population and number of dengue cases of which county, it's necessery to merge these informations and create an incidence rate.

## Merging socio-environmental and estimated population
"""

mergepop = pd.merge(pop, socio, left_on="NOME DO MUNIC√çPIO", right_on="municipio").drop(
    ["COD. UF", "NOME DO MUNIC√çPIO"], axis=1).rename({
        "POPULA√á√ÉO ESTIMADA": "pop estimada"
    }, axis=1)
mergepop

"""## Merging and creating the incidence rate"""

merge_prev = pd.merge(mergepop, dengue, on='municipio', how='left')
merge_prev['total '].fillna(0, inplace=True)
merge_prev["incidence rate"] = merge_prev['total '] / merge_prev['pop estimada']
merge_prev['incidence rate'] = merge_prev['incidence rate'].astype(float)
merge_prev['incidence rate'] = round(merge_prev['incidence rate'], 3)
merge_prev.sort_values('incidence rate', ascending=False)

"""#**Clustering the variables and creating a socio-environmental deficiency index**

Before building a predictive model, we create a socio-environmental deficiency index through principal component analysis (PCA).

Subset including variables to be clustered
"""

subset = merge_prev[["lixo", "esgoto",	"estudo",	"salario"]]

"""Principal component analysis"""

pca = PCA()
pca.fit(subset).explained_variance_ratio_

kmeans = KMeans(n_clusters=3, random_state=0).fit(subset)

cluster_map = pd.DataFrame()
cluster_map['data_index'] = subset.index.values
cluster_map['cluster'] = kmeans.labels_

cluster_map['cluster'].value_counts()
subset.loc[cluster_map['cluster'] == 2].describe()

merge_prev['cluster'] = kmeans.labels_

"""Cluster x incidence rate plot"""

fig, ax = plt.subplots(figsize=(5, 5), dpi=100)
plt.scatter(merge_prev['cluster'], merge_prev['incidence rate'])

"""Analysing the avarage and variance of varible values, we can index which cluster and define it's socio-environmental deficiency."""

merge_prev[merge_prev['cluster'] == 0][["lixo", "esgoto",	"estudo",	"salario"]].describe()

merge_prev[merge_prev['cluster'] == 1][["lixo", "esgoto",	"estudo",	"salario"]].describe()

merge_prev[merge_prev['cluster'] == 2][["lixo", "esgoto",	"estudo",	"salario"]].describe()

merge_prev[["lixo", "esgoto",	"estudo",	"salario"]].describe()

"""Mapping the clusters"""

mapping = {
    0: 1, 
    1: 2, 
    2: 0, 
}
merge_prev['cluster'] = merge_prev['cluster'].apply(lambda x: mapping[x])

"""Groups of risk"""

low_risk = merge_prev[merge_prev['cluster'] == 0]
medium_risk = merge_prev[merge_prev['cluster'] == 1]
high_risk = merge_prev[merge_prev['cluster'] == 2]

"""Histograms of the incidences of risk groups"""

fig,axes = plt.subplots(3,1,figsize=(10,10))

axes[0].hist(low_risk['incidence rate'], bins=50)
axes[0].set_title("Low Risk")

axes[1].hist(medium_risk['incidence rate'], bins=50)
axes[1].set_title("Medium Risk")

axes[2].hist(high_risk['incidence rate'], bins=50)
axes[2].set_title("High Risk")

plt.tight_layout();

"""#**Bayesian modeling**

##Student T Likelihood

MCMC diagnostics:

*   From the following trace plot, we can visually get the plausible values of ùúá from the posterior.
*   We should compare this result with those from the the result we obtained analytically.
"""

with pm.Model() as model_t:
  
  Œº = pm.Uniform('Œº', lower=0, upper=0.15)
  œÉ = pm.HalfNormal('œÉ', sd=10)
  ŒΩ = pm.Exponential('ŒΩ', 1/1)
  y = pm.StudentT('y', mu=Œº, sd=œÉ, nu=ŒΩ, observed=merge_prev['incidence rate'])
  trace_t = pm.sample(2000, tune=2000)

az.plot_trace(trace_t[:1000], var_names = ['Œº']);

merge_prev['incidence rate'].mean()

"""*   The left plot shows the distribution of values collected for ùúá. What we get is a measure of uncertainty and credible values of ùúá between 0.016 and 0.024.
*   It is obvious that samples that have been drawn from distributions that are significantly different from the target distribution.

## Poisson Distribution
"""

with pm.Model() as model_p:
  
  Œº = pm.Uniform('Œº', lower=0, upper=0.15)
  
  ## Define Poisson likelihood
  y = pm.Poisson('y', mu=Œº, observed=merge_prev['incidence rate'].values)
  trace_p = pm.sample(2000, tune=2000)

az.plot_trace(trace_p);

"""The measure of uncertainty and credible values of ùúá is between 0.00 and 0.07. Not suitable for our data.

##Inverse Gaussian Distribution
"""

with pm.Model() as model_w:
    
    #prior_sig = pm.Exponential("prior_sig", lam=0.9) 
    Œº = pm.Uniform("mu", lower=0, upper=0.15)
    
    like = pm.Wald(
        "likelihood", 
        mu=Œº,
        lam=0.09, #0.07 ou 0.09
        observed=merge_prev['incidence rate'],
    )

with model_w:
  trace_wald = pm.sample(2000, tune=2000)

az.plot_trace(trace_wald, var_names=['mu'])

"""The measure of uncertainty and credible values of ùúá is between 0.024 and 0.036 minutes, and it is very closer to the target sample mean.

###Posterior checks
"""

ppc = pm.sample_posterior_predictive(trace_wald, samples=1000, model=model_w)
_, ax = plt.subplots(figsize=(10, 5))
ax.hist([n.mean() for n in ppc['likelihood']], bins=19, alpha=0.5)
ax.axvline(merge_prev['incidence rate'].mean(), label='Sample Mean')
ax.set(xlabel='Mean', ylabel='Frequency')
plt.legend()

"""These confirm that Inverse Gaussian Distribution is suitable for our data"""

_ = pm.autocorrplot(trace_wald)

"""We want the autocorrelation to fall as the x-axis increases in the graph. Because this indicates a low degree of correlation between our samples.

Our Inverse Gaussian model samples dropped to low values of autocorrelation, which is a good sign.
"""

y_ppc = pm.sample_posterior_predictive(trace_wald, 100, model_w, random_seed=123) 
y_pred = az.from_pymc3(trace=trace_wald, posterior_predictive=y_ppc) 
az.plot_ppc(y_pred, figsize=(10, 5), mean=False) 
plt.xlim(0, 0.15);

"""Using the Inverse Gaussian distribution in our model leads to predictive samples that seem to better fit the data in terms of the location of the peak of the distribution and also its spread.

The single (black) line is a kernel density estimate (KDE) of the data and the many purple lines are KDEs calculated from each of the 100 subsequent predictive samples. The blue lines reflect the uncertainty we have about the inferred distribution of the predicted data.
From the graph above, we can consider the scale of an Inverse Gaussian distribution as a reasonable practical proxy for the standard deviation of the data.

### Odds ratio

Does socio-envorinmental deficiency index of county affects incidence rate of dengue? To do it we will use the concept of odds, and we can estimate the odds ratio of socio-envorinmental vulnerability like this:

First we do a hierarquical model with Inverse Gaussian Distribution
"""

with pm.Model() as wald_model_h:
    
    prior_sig = pm.Exponential("prior_sig", lam=0.9) 
    mu = pm.HalfNormal("mu", sd=prior_sig, shape=(3,))
    
    like = pm.Wald(
        "likelihood", 
        mu=mu[merge_prev['cluster']],
        lam=0.09, #0.07 ou 0.09
        observed=merge_prev['incidence rate'],
    )

with wald_model_h:
  trace_wald_h = pm.sample(2000, tune=2000)

az.plot_posterior(trace_wald_h, var_names=['mu'])

"""Then we can estimate odds ratio and percentage effect for all the variables."""

stat_df = pm.summary(trace_wald_h)
stat_df['odds_ratio'] = np.exp(stat_df['mean'])
stat_df['percentage_effect'] = 100 * (stat_df['odds_ratio'] - 1)
stat_df.sort_values('odds_ratio', ascending=False)

"""We can interpret percentage_effect along those lines: ‚Äú With a one unit increase in education, the odds of subscribing to a term deposit increases by 8%. Similarly, for a one unit increase in euribor3m, the odds of subscribing to a term deposit decreases by 43%, while holding all the other independent variables constant.‚Äù"""